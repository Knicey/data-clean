---
title: "Homework 3"
author: "Nathan Yang"
date: "9-15-2024"
output:
  pdf_document: default
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("pacman")) {
install.packages("pacman")
library(pacman)
}
p_load(RecordLinkage, blink, knitr, textreuse, tokenizers, devtools, cora, ggplot2, dplyr)

data(cora) # load the cora data set
#dim(cora)
data(cora_gold) 
#head(cora_gold) # contains pairs of records that are true matches.
#dim(cora_gold)
data(cora_gold_update) # contains a true unique identifier 
#dim(cora_gold_update) 
#length(unique(cora_gold_update$unique_id)) 
```

Consider the cora citation data set and load the data set with an column id as we did in class. Code is provided below.

```{r, cache=TRUE, echo=TRUE}
# get only the columns we want
# number of records
n <- nrow(cora)
# create id column
dat <- data.frame(id = seq_len(n))  
# get columns we want
dat <- cbind(dat, cora[, c("title", "authors", "journal")]) 
```

Perform the LSH approximation as we did in class using the `textreuse` package via the functions `minhash_generator` and `lsh` (so we don't have to perform it by hand). Again, this code is provided for you given that it was done in class to make it a bit easier. Feel free to play around with this on your own. We will assume that m = 360, b = 90, and the number of shingles is 3 for this assignment.

## Find the number of buckets or bands to use

```{r show-package-lsh, echo=TRUE, cache=TRUE, warnings=FALSE}
p_load(numbers) 
m <- 360
bin_probs <- expand.grid(s = c(.25, .75), h = m, b = divisors(m))
#bin_probs
# choose appropriate num of bands and number of random permutations m (tuning parameters)
bin_probs$prob <- apply(bin_probs, 1, function(x) lsh_probability(x[["h"]], x[["b"]], x[["s"]]))
# plot as curves
ggplot(bin_probs) +
  geom_line(aes(x = prob, y = b, colour = factor(s), group = factor(s)), linewidth = 2) +
  geom_point(aes(x = prob, y = b, colour = factor(s)), linewidth = 3) +
  xlab("Probability") +
  scale_color_discrete("s")

# create the minhash function
minhash <- minhash_generator(n = m, seed = 02082018)
b <- 90
```

## Build corpus and perform shingling

```{r show-package-lsh-1, echo=TRUE, cache=TRUE}
head(dat)
# build the corpus using textreuse
docs <- apply(dat, 1, function(x) paste(x[-1], collapse = " ")) # get strings
names(docs) <- dat$id # add id as names in vector
corpus <- TextReuseCorpus(text = docs, # dataset
                          tokenizer = tokenize_character_shingles, n = 3, 
                          simplify = TRUE, # shingles
                          progress = FALSE, # quietly
                          keep_tokens = TRUE, # store shingles
                          minhash_func = minhash) # use minhash
head(minhashes(corpus[[1]]))
length(minhashes(corpus[[1]]))
```

Note that all our records are now represented by 360 randomly selected and hashed shingles. Comparing these shingles are equivalent to finding the Jaccard similarity of all the record pairs. We still have an issue of all the pairwise comparison.

## Find buckets, candidate records, and Jaccard similarity

Now, we find the buckets, candidates records, and calculate the Jaccard similarity for the candidate records (in the buckets)

```{r show-package-lsh-2, echo=TRUE, cache=TRUE}

# perform lsh to get buckets
buckets <- lsh(corpus, bands = b, progress = FALSE)

# grab candidate pairs
candidates <- lsh_candidates(buckets)

# get Jaccard similarities only for candidates
lsh_jaccard <- lsh_compare(candidates, corpus, 
                           jaccard_similarity, progress = FALSE)
head(buckets)
dim(buckets)
length(unique(buckets))
head(lsh_jaccard)
```

We now plot the Jaccard similarities that are candidate pairs (under LSH)

```{r, lsh-plot,echo=FALSE}
# plot jaccard similarities that are candidates
qplot(lsh_jaccard$score)
```

1.  Calculate the reduction ratio from the total number of record comparisons ($N$ choose 2) compared to those under locality sensitive hashing (above).

    -   Naive Approach: $N(N-1)/2$. Since have have `r n` records, the naive\
        approach would result in `r n*(n-1)/2` comparisons.
    -   Locality Sensitive Hashing brought the number of comparisons down to `r dim(candidates)[1]`.
    -   Reduction Ratio: `r ((n*(n-1)/2) - dim(candidates)[1]) / (n*(n-1)/2)/ (n*(n-1)/2)`.

2.  Find the pairwise precision and recall under locality sensitive hashing. There are two places where we have ground truth. Note that cora_gold contains record pairs that are true matches; cora_gold_update contains a unique identifer alternatively. You will need to write your own code for this.

    -   Precision Calculation:

    ```{r}

    # get the candidate pairs
    candidate_pairs <- data.frame(id1 = candidates$a, id2 = candidates$b)
    # get the true matches that are in the candidate pairs
    correct_true_matches <- merge(cora_gold, candidate_pairs, by.x = c("id1", "id2"), by.y = c("id1", "id2"))
    # calculate precision
    precision <- nrow(correct_true_matches) / nrow(candidate_pairs)
    precision
    recall <- nrow(correct_true_matches) / nrow(cora_gold)
    recall
    ```

3.  We can further reduce the problem by filtering out candidate pairs of records below a threshold $t$ that are unlikely to be matches. For example, assume $t = 0.8.$ Filter out all record pairs below the threshold of $0.8.$ We will call this locality sensitive hashing with filtering/thresholding.

    -   Filter out all record pairs below the threshold of 0.8.

    ```{r}
    # filter out all record pairs below the threshold of 0.8
    lsh_filtered <- lsh_jaccard[lsh_jaccard$score >= 0.8, ]
    head(lsh_filtered)
    ```

4.  Under lsh with t = 0.8, re-compute the precision, recall, and reduction ratio.

    -   Recalculate reduction ratio

    ```{r}
    reduction_ratio <- (n * (n-1)/2 - nrow(lsh_filtered)) / (n * (n-1)/2)
    reduction_ratio

    # get the candidate pairs
    candidate_pairs <- data.frame(id1 = lsh_filtered$a, id2 = lsh_filtered$b)
    correct_true_matches <- merge(cora_gold, candidate_pairs, by.x = c("id1", "id2"), by.y = c("id1", "id2"))
    # calculate precision
    precision <- nrow(correct_true_matches) / nrow(candidate_pairs)
    precision
    recall <- nrow(correct_true_matches) / nrow(cora_gold)
    recall
    ```

    With the filtered set of pairs, our reduction ratio is now `r reduction_ratio`, precision `r precision`, and recall `r recall`

5.  

<!-- -->

i.  Describe what the blocks look like from this method?

```{r}
buckets <- buckets |>
  group_by(buckets) |>
  summarise(
    n = n()
  )

ggplot() +
  geom_bar(aes(x = buckets$n)) 
```

The distribution of block sizes is a very right-skewed histogram showing that most blocks are very small containing only a few records.

ii. Are they non-overlapping or overlapping? They are overlapping since documents can be mapped to multiple buckets

iii. Describe some advantages and disadvantages of this method that you see from using it practically. Some advantages include a large reduction ratio, and high recall. Some disadvantages include needing further refinement to maximize precision.
