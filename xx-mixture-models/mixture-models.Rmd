
---
title: "The Two Component Mixture Model"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Agenda
===
- Two Component Mixture Model
- Latent Variable 

What will you learn in this lecture
===

- Importance of mixture models 
- Simple illustrations of mixture models
- We will learn about the two component mixture model
- Will learn about latent variables
- Two component mixture model 
- EM algorithm 



Importance
===

- First proposed by Karl Pearson (1984) and analyzed on crab data. 
- Applications: "agriculture, astronomy, bioinformatics, biology, economics, engineering, genetics, imaging, marketing, medicine, neuroscience, psychiatry, and psychology, among many other fields in the biological, physical, and social sciences". McLachlan et. al (2019). 
- One of the methods in machine learning is **topic modeling**, which identifies "topics" in collections of documents/webpages. 
- Topic modeling relies on mixtures models. 

Motivation
===

- Suppose we want to simulate the price of a randomly chosen book. 

- Paperbacks are often cheaper than hardbacks, so let's model them separately. 

- Model the price of a book as a mixture model.

- There will be two components (or clusters) in our model -- one for paperbacks and one for hardbacks. 

Model 
===

- Paperback distribution: $N(9,1)$
- Hardback distribution: $N(20,2)$
- Assume that there's a there is a 50\% chance of choosing a paperback and 50\% of choosing hardback.


Motivation
===

```{r, echo=FALSE}
numSamples <- 5000
prices      <- numeric(numSamples)
for(i in seq_len(numSamples)) {
  # draw latent component
  z.i <- rbinom(1,1,0.5)
  # two conditions for each normal distribution based upon the latent component
  if(z.i == 0) prices[i] <- rnorm(1, mean = 9, sd = 1)
  else prices[i] <- rnorm(1, mean = 20, sd = 1)
}
hist(prices)
```

Motivation
===

- Are the prices of books unimodal or bimodal? 
- Suppose you would want to predict the price of a book. Would its distribution be Normal or something else based on the the histogram. 
 
Motivation
===


```{r, echo=FALSE}
mu.pb   <- 9
sd.pb   <- 1
mu.hb   <- 20
sd.hb   <- 1

sample.pts <- seq(5, 25, by=0.1)
density_pb   <- dnorm(sample.pts, mean=mu.pb, sd=sd.pb)
density_hb <- dnorm(sample.pts, mean=mu.hb, sd=sd.hb)

plot(sample.pts, density_pb, col='red', type='l', xlab="Price ($)", ylab="Density", lty=2)
lines(sample.pts, density_hb, col='blue', type='l', lty=2)
lines(sample.pts, .5*density_hb + .5*density_pb, col='black', type='l', lwd=2)

legend('topright', c('paperback', 'hardback', 'all books'), col=c('red', 'blue', 'black'), lty=c(2,2,1), lwd=c(1,1,2), cex=0.7)
```

Motivation
===

Now assume our data are the heights of students at University X.

Assume the height of a randomly chosen male is normally distributed with a mean equal to $5'9$ and a standard deviation of $2.5$ inches.

Assume the height of a randomly chosen female is $N(5'4, 2.5)$. 

Assume that 75\% of the population is female and 25\% is male. 

Motivation
===

```{r, echo= FALSE}
numSamples <- 5000
heights      <- numeric(numSamples)
for(i in seq_len(numSamples)) {
  z.i <- rbinom(1,1,0.75)
  if(z.i == 0) heights[i] <- rnorm(1, mean = 69, sd = 2.5)
  else heights[i] <- rnorm(1, mean = 64, sd = 2.5)
}
hist(heights)
```
 
 
Motivation
===

The histogram is now unimodal. 

Are heights normally distributed (assuming this model)? Let's invesitgate! 

Motivation
===

```{r, echo=FALSE}
mu.male   <- 69
sd.male   <- 2.5
mu.female <- 64
sd.female <- 2.5

sample.pts     <- seq(55, 80, by=0.1)
density_male   <- dnorm(sample.pts, mean=mu.male, sd=sd.male)
density_female <- dnorm(sample.pts, mean=mu.female, sd=sd.female)

plot(sample.pts, density_male, col='red', type='l', xlab="Height (inches)", ylab="Density", lty=2)
lines(sample.pts, density_female, col='blue', type='l', lty=2)
lines(sample.pts, .75*density_female + .25*density_male, col='black', type='l', lwd=2)

legend('topright', c('male', 'female', 'population'), col=c('red', 'blue', 'black'), lty=c(2,2,1), lwd=c(1,1,2), cex=0.7)
```

Motivation
===

The Gaussian mixture model is unimodal because there is so much overlap between the two densities. 

In this example, observe that the population density is not symmetric, and therefore not normally distributed. 



Goal
===

The goal of this module is to introduce **mixture models**, which are commonly used in applications in classical and modern machine learning.

We will do this using a **latent variable**.

Background 
===

A **latent variable** is the true version of the state of a random variable that is unknown and not directly observed.\footnote{We will not delve into the properties of latent variables in this course.}

Mixture models can be viewed as probabilistic clustering
===

- Mixture models put similar data points into "clusters".

- This is appealing as we can potentially compare different probabilistic
clustering methods by how well they predict (under cross-validation). We will not explore this in this particular lecture. 

- This contrasts other methods such as k-means and hierarchical clustering as they produce clusters (and not predictions), so it's difficult to test if they are correct/incorrect.\footnote{Explore looking at these on your own and see if you can determine their limitations practically, compared to other machine learning models.} 





Two-component mixture model
===
Assume that both mixture components (female and male) have the same precision,  $\lambda$, which is fixed and known. 
\vfill
Let $\pi$ be the mixture proportion for the first component. 
\vfill

Then the two-component Normal mixture model is:
\begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)
\end{align}
where $F(\mu,\pi)$ is the distribution with p.d.f. 
$$ f(x|\mu,\textcolor{blue}{\pi}) = (1-\textcolor{blue}{\pi})\N(x\mid \mu_0,\lambda^{-1}) + \textcolor{blue}{\pi}
\N(x\mid \mu_1,\lambda^{-1}).$$


<!-- In the two-component mixture model, it assumes each observation is generated from one of two -->
<!--  mixture components, where $\pi$ is the mixture proportion for the first component and $1-\pi$ is the mixture proportion for the second component. -->





Likelihood
===
The likelihood is
\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}

Likelihood
===

\textcolor{blue}{What do you notice about the likelihood function?}

\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big].
\end{align*}


Likelihood
===
The **likelihood** is very complicated function of $\mu$ and $\pi$. 

\vspace*{1em}

This makes working with it directly to find the MLE (or other estimates) difficult.

\vspace*{1em}

Thus, we will rewrite the likelihood using **latent variables**. 

Latent allocation variables to the rescue!
===
Define an equivalent model that includes latent ``allocation'' variables $Z_1,\ldots,Z_n.$
\vskip 1em

These indicate which mixture component each data point
comes from--that is, $Z_i$ indicates whether subject $i$ is from component 1 or 2.
\begin{align}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
 & Z_1,\ldots,Z_n \mid \mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)
\end{align}

How can we check that the latent allocation model is equivalent to our original model? 



Equivalence of both models 
===
Recall
\begin{align*}
 & X_i \mid \mu, Z \sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)
\end{align*}

This is equivalent to the model above, since
\begin{align}
    &p(x_i|\mu,\pi) \\
    &= p(x_i|Z_i=0,\mu,\pi)\Pr(Z_i=0|\mu,\pi)
                    + p(x_i|Z_i=1,\mu,\pi)\Pr(Z_i=1|\mu,\pi) \\
            &= (1-\pi)\N(x_i|\mu_0,\lambda^{-1}) + \pi\N(x_i|\mu_1,\lambda^{-1})\\
            &= f(x_i|\mu,\pi),
\end{align}
and thus it induces the same distribution on $(x_{1:n},\mu,\pi)$. The latent model is considerably easier to work with mathematically! 

Extension to k-components
===

Assume we observe $X_1,\ldots,X_n$ and that each $X_i$ is sampled from one of $K$ **mixture components**.  

Associated with each random variable $X_i$ is a latent variable (or label) $Z_i \in \{1,\ldots,K\}$ which indicates which component $X_i$ came from. 


Notation
===

Let $\pi_k$ be called **mixture proportions** or **mixture weights**, which represent the probability that $X_i$ belongs to the $k$-th mixture component. 
\vfill

The mixture proportions are non-negative and they sum to one, $\sum_{k=1}^K \pi_k = 1$. 
\vfill

We call $P(X_i \mid Z_i=k)$ the **mixture component**, and it represents the distribution of $X_i$ assuming it came from component $k$. 

Law of Total Probability
===

From the law of total probability, it follows that 

\begin{align}
P(X_i = x) &= \sum_{k=1}^K P(X_i=x|Z_i=k)\underbrace{P(Z_i=k)}_{\pi_k} \\
&= \sum_{k=1}^K P(X_i=x|Z_i=k)\pi_k \\
&= \sum_{k=1}^K \pi_k P(X_i=x|Z_i=k)
\end{align}


Mixture Models
===

For discrete random variables these mixture components can be any probability mass function $p(\cdot \mid Z_{k}).$ 

For continuous random variables they can be any probability density function $f(\cdot \mid Z_{k})$. 

The corresponding pmf and pdf for the mixture model are:

$$p(x) =  \sum_{k=1}^{K}\pi_k p(x \mid Z_{k})$$
$$f_{x}(x) = \sum_{k=1}^{K}\pi_k f_{x \mid Z_{k}}(x \mid Z_{k}) $$
 

Likelihood
===

The likelihood of observing independent samples $X_1,\ldots,X_n$ with mixture proportion vector $\pi=(\pi_1, \pi_2,\ldots,\pi_K)$ is therefore:

$$L(X_1, \ldots, X_n \mid \pi) = \prod_{i=1}^n P(X_i|\pi) = \prod_{i=1}^n\sum_{k=1}^K P(X_i|Z_i=k)\pi_k$$

Estimation
===

Now assume we are in the Gaussian mixture model setting where the $k$-th component is $N(\mu_k, \sigma^2)$ and the mixture proportions are $\pi_k$. 

How can we estimate $\{\mu_k,\sigma^2, \pi_k\}$ from the observed data $X_1,\ldots,X_n$? 

Solution: EM Algorithm. 

Why? The MLE is not possible to find in closed form. Think about why this is the case. 



Conditional and marginal distributions
===

Recall that the conditional distribution $X_i|Z_i = k \sim N(\mu_k, \sigma_k^2),$ where $\pi_k = P(Z_i = k).$

The marginal distribution of $X_i$ is:
\begin{align}
P(X_i = x) &= \sum_{k=1}^K P(Z_i = k) P(X_i=x | Z_i = k) \\
&= \sum_{k=1}^K \pi_k N(x \mid \mu_k, \sigma_k^2)
\end{align}

Joint distribution
===

The joint probability of observations $X_1,\ldots,X_n$ is 

$$P(X_1=x_1,\ldots,X_n=x_n) = \prod_{i=1}^n \sum_{k=1}^K \pi_k N(x_i \mid \mu_k, \sigma_k^2)$$

Exercise
===

Show that 

\begin{align}
&\log P(X_1, \ldots, X_n \mid \mu_1, \ldots, \mu_K) \\
&= \log \prod_{i=1}^n P(x_i \mid \mu_1, \ldots, \mu_K) \\
&= \sum_{i=1}^n \log [
\sum_{k=1}^K P(x_i \mid \pi_k, \mu_1, \ldots, \mu_K) \pi_k
]
\end{align}


Background
===

Recall that 

$$\frac{\partial \log f(x)}{\partial dx} = \frac{1}{f(x)}\frac{\partial f(x)}{\partial dx}.$$

Exercise
===

Show that 

\begin{align}
&\frac{\partial \log P(X_1, \ldots, X_n \mid \mu_1, \ldots, \mu_K)}{\partial  \mu_k} \\
&= \sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) \frac{(x_i - \mu_k)}{\sigma}
\end{align}

This implies that 

$$\mu_k = \frac{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) x_i}
{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)},$$
which is a non-linear equation of the $\mu_k$'s.

Intuition of EM
===

$$\mu_k = \frac{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K) x_i}
{\sum_{i=1}^n P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)},$$

- E-step: If for each $x_i$ we knew that for each $\pi_k$ the prob. that $\mu_k$  was in component 
$\pi_k$ is $P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K).$ Then we could compute $\mu_k.$

- M-step: If we knew each $\mu_k$, then we could compute $P(\pi_k \mid x_i, \mu_1, \ldots, \mu_K)$ for each $\mu_k$ and $x_i$

EM Algorithm
===

Initalize all the unknown parameters. On iteration $t$, let the estimates be
$\lambda^{(t)} = \{\mu_1^{(t)}, \ldots, \mu_k^{(t)} \}$

1. E-Step: 

\begin{align}
P(\pi_k \mid x_i, \lambda^{(t)}) 
&=
\frac{P(\pi_k \mid x_i, \lambda^{(t)}) x_i}
{P(\pi_k \mid x_i, \lambda^{(t)})}
\end{align}

2. M-Step:

\begin{align}
\mu_k^{(t+1)} &= \frac{\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t)}) x_i}
{\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t)})}
\end{align}



<!-- and $p_k^{(t+1)} = \frac{\sum_{i=1}^n P(\pi_k \mid x_i,  \lambda^{(t))}{n}$ -->



Exercise
===
Assume our mixture components are fully specified Gaussian distributions with $K=2$ components:

\begin{align}
X_i \mid Z_i = 0 &\sim N(5, 1.5) \\
X_i \mid Z_i = 1 &\sim N(10, 2)
\end{align}

Let the true mixture proportions be $P(Z_i = 0) = 0.25$ and $P(Z_i = 1) = 0.75$, respectively. 

Exercise
===

1. Simulate data from the mixture model on the previous slide, which should produce the following histogram. 

```{r, echo= FALSE}
# set mu and sigma
mu.true    = c(5, 10)
sigma.true = c(1.5, 2)

# Calculate Z
Z = rbinom(500, 1, 0.75)

# sample values from the mixture model 
X <- rnorm(10000, mean=mu.true[Z+1], sd=sigma.true[Z+1])
hist(X,breaks=15)
```

Exercise

2. Write a function to compute the log-likelihood of incomplete data (assuming the parameters are known). 

The log-likelihood for the incomplete data ($X$) is as follows

$$l(\theta) = \sum_{i=1}^n \log \left(
\sum_{k=1}^2
\pi_k N(x_i \mid \mu_k, \sigma_k^2)
\right).$$

Exercise
===
Compute the likelihood $P(X_i|Z_i=0)$
and $P(X_i|Z_i=1)$ and store these in a matrix $L.$
 
Exercise
===

Implement the E and M step in a function called `emIteration`. Then evaluate the EM and verify that your estimates are 0.29 and 0.71, respectively. 

Plot the incomplete log-likelihood versus the iteration. What do you observe regarding its behavior. 

R packages for mixture models
===

- The `mclust` package (http://www.stat.washington.edu/mclust/) is standard for
Gaussian mixtures. 

- The `mixtools` considers classic parametric densities, mixtures of regressions, and some non-parametric mixtures. 

 




